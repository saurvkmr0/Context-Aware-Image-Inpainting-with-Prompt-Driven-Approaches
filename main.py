# -*- coding: utf-8 -*-
"""inpainting_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJAIdQlxpF0M1180RMJ_f-NVkTD0c-AW
"""

# run the code with a T4(gpu) runtime environment in googlecloab
# regference
# https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg_GRADIO_WEB_UI.ipynb

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

! git lfs install
! git clone https://github.com/timojl/clipseg

! pip install diffusers -q

! pip install transformers -q -UU ftfy gradio

! pip install git+https://github.com/openai/CLIP.git -q

from huggingface_hub import notebook_login

notebook_login()

# Commented out IPython magic to ensure Python compatibility.
# %cd clipseg

# from google.colab import drive

# # Mount Google Drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Workspace/CEVI_MINOR
!cp -r weights /content/clipseg

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/clipseg

! ls

import torch
import requests
import cv2
from models.clipseg import CLIPDensePredT
from PIL import Image
from torchvision import transforms
from matplotlib import pyplot as plt

from io import BytesIO

from torch import autocast
import requests
import PIL
import torch
from diffusers import StableDiffusionInpaintPipeline as StableDiffusionInpaintPipeline

#! git lfs install

# load model
model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64)
model.eval();

# non-strict, because we only stored decoder weights (not CLIP weights)
model.load_state_dict(torch.load('/content/clipseg/weights/rd64-uni.pth', map_location=torch.device('cuda')), strict=False);

device = "cuda"
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    revision="fp16",
    torch_dtype=torch.float16,
    use_auth_token=True
).to(device)

# or load from URL...
image_url = 'https://assets.architecturaldigest.in/photos/63fdd15ca3b6ca40fb58057b/16:9/w_1615,h_908,c_limit/Feng%20Shui-%204%20ways%20to%20decorate%20your%20dining%20room%20to%20attract%20abundance.jpg' #'https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimage.tmdb.org%2Ft%2Fp%2Foriginal%2F72xYNWRTVMDiKVa6SVu6EY0S9Np.jpg' #'https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png'
input_image = Image.open(requests.get(image_url, stream=True).raw)

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.Resize((512, 512)),
])
img = transform(input_image).unsqueeze(0)

# input image references
# https://static.toiimg.com/photo/84517409.cms
# https://p0.pxfuel.com/preview/440/625/255/cosmos-flower-cosmos-plant-pink-flowers-pictures-of-flowers-royalty-free-thumbnail.jpg
# https://www.bhg.com/thmb/1J6WJtvucECFN6LHN978Vudejs4=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/BHG_PTSN18751_preview1-3140b2dd5f21426790e20da2857dd847.jpg

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

input_image.convert("RGB").resize((512, 512)).save("init_image.png", "PNG")

from matplotlib import pyplot as plt
plt.imshow(input_image, interpolation='nearest')
plt.show()

prompts = ['mirror']

# predict
with torch.no_grad():
    preds = model(img.repeat(len(prompts),1,1,1), prompts)[0]

# visualize prediction
_, ax = plt.subplots(1, 5, figsize=(15, 4))
[a.axis('off') for a in ax.flatten()]
ax[0].imshow(input_image)
[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(len(prompts))];
[ax[i+1].text(0, -15, prompts[i]) for i in range(len(prompts))];

filename = f"mask.png"
plt.imsave(filename,torch.sigmoid(preds[0][0]))

img2 = cv2.imread(filename)

img2 = cv2.imread(filename)

gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

(thresh, bw_image) = cv2.threshold(gray_image, 100, 255, cv2.THRESH_BINARY)

# For debugging only:
cv2.imwrite(filename,bw_image)

# fix color format
cv2.cvtColor(bw_image, cv2.COLOR_BGR2RGB)

Image.fromarray(bw_image)

init_image = Image.open('init_image.png')
mask = Image.open('mask.png')

mask

from diffusers import StableDiffusionInpaintPipelineLegacy

with autocast("cuda"):
    images = pipe(prompt="window", image=init_image, mask_image=mask, strength=0.95).images

images[0]

